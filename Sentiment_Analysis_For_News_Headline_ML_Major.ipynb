{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis For News Headline- ML Major",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B94bs-x_FVP"
      },
      "source": [
        "#Stage 1 : Installing and Importing requires libraries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwf5JegWeNPV"
      },
      "source": [
        "# installing requirements\r\n",
        "!pip install contractions\r\n",
        "!pip install vaderSentiment\r\n",
        "!pip install gitpython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU9PFagCEY4L"
      },
      "source": [
        "# importing needed libraries\r\n",
        "\r\n",
        "# for Web Scraping\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "# for general data processing \r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "from git import Repo\r\n",
        "\r\n",
        "# for data preprocessing\r\n",
        "import nltk\r\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\r\n",
        "import re\r\n",
        "import contractions\r\n",
        "import string\r\n",
        "\r\n",
        "# for building model\r\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.svm import SVC\r\n",
        "\r\n",
        "# for checking accuracy of model\r\n",
        "from sklearn.metrics import accuracy_score,classification_report, confusion_matrix\r\n",
        "\r\n",
        "# for saving and running saved model\r\n",
        "import pickle\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7E9jJ7D_fV7"
      },
      "source": [
        "# Stage 2: Web Scraping Data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-ZhrfXQE1fK"
      },
      "source": [
        "# Getting list of categories to scrap data from \r\n",
        "url = 'https://inshorts.com/en/read/'\r\n",
        "urls = []\r\n",
        "categories = []\r\n",
        "news_categories = []\r\n",
        "soup=BeautifulSoup(requests.get(url).content)\r\n",
        "datastr = str(soup.find_all('ul', class_=[\"category-list\"]))\r\n",
        "datastr=datastr[datastr.index(\"<a href=\\\"/en/read/\"):datastr.index(\"</ul>\")]\r\n",
        "categories=datastr.split(\"</a>\")\r\n",
        "for i in range(len(categories)):\r\n",
        "  a=categories[i]\r\n",
        "  if(len(a)>17):\r\n",
        "    j=a.index('\\\" ',8)\r\n",
        "    if(j>17):\r\n",
        "      if(i==0):\r\n",
        "        news_categories.append(a[18:j])\r\n",
        "      else:\r\n",
        "        news_categories.append(a[19:j])\r\n",
        "for s in news_categories:\r\n",
        "  urls.append(url+s+'/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01ZDDKnlN138"
      },
      "source": [
        "# building dataset by scraping data from inshorts \r\n",
        "def build_dataset(urls):\r\n",
        "  news_data=[]\r\n",
        "  for u in urls:\r\n",
        "    soup=BeautifulSoup(requests.get(u).content)\r\n",
        "    category=u[0:len(u)-1]\r\n",
        "    category=category[category.rindex('/')+1:len(category)]\r\n",
        "    news_article =[{'news_headline': headline.find('span', attrs={\"itemprop\":\"headline\"}).string,\r\n",
        "                    'news_article': article.find('div', attrs={\"itemprop\":\"articleBody\"}).string,\r\n",
        "                    'news_category': category}\r\n",
        "                   for headline,article in zip(soup.find_all('div',class_=[\"news-card-title news-right-box\"]),\r\n",
        "                                               soup.find_all('div',class_=[\"news-card-content news-right-box\"]))\r\n",
        "                   ]\r\n",
        "    news_article = news_article[0:20]\r\n",
        "    news_data.extend(news_article)\r\n",
        "  df=pd.DataFrame(news_data)\r\n",
        "  df=df[['news_headline','news_article', 'news_category']]\r\n",
        "  return df\r\n",
        "df=build_dataset(urls)\r\n",
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keki9SxZ_mSd"
      },
      "source": [
        "# Stage 3 : Data Pre-Processing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpDvN7XWpl96"
      },
      "source": [
        "# downloading stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\r\n",
        "stopword_list.remove('no')\r\n",
        "stopword_list.remove('not')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju9VVplsrHa9"
      },
      "source": [
        "# function to remove HTML tag\r\n",
        "def html_tag(text):\r\n",
        "  soup=BeautifulSoup(text,\"html.parser\")\r\n",
        "  new_text = soup.get_text()\r\n",
        "  return new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68rKBzEOspiF"
      },
      "source": [
        "# function to remove contractions\r\n",
        "def con(text):\r\n",
        "  expand = contractions.fix(text)\r\n",
        "  return expand\r\n",
        "con(\"Y'all can't expand I'd think\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsErEZTbtqy8"
      },
      "source": [
        "# remove special characters\r\n",
        "def remove_sp(text):\r\n",
        "  pattern= r'[^A-Za-z0-9\\s]'\r\n",
        "  text= re.sub(pattern,'',text)\r\n",
        "  return text\r\n",
        "\r\n",
        "remove_sp(\"well it is fun !! what @ do you think.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QniGMTIvZ2E"
      },
      "source": [
        "# function to remove stop words\r\n",
        "tokenizer = ToktokTokenizer()\r\n",
        "def remove_stopwords(text):\r\n",
        "  tokens = tokenizer.tokenize(text)\r\n",
        "  tokens = [token.strip() for token in tokens]\r\n",
        "  filtered_tokens = [token for token in tokens if token not in stopword_list]\r\n",
        "  filtered_text= ' '.join(filtered_tokens)\r\n",
        "  return filtered_text\r\n",
        "\r\n",
        "remove_stopwords(\"The, and , if are all stop words and even not\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8SkY9md8rVC"
      },
      "source": [
        "# data pre-processing\r\n",
        "df.news_headline = df.news_headline.apply(lambda x:x.lower())\r\n",
        "df.news_article = df.news_article.apply(lambda x:x.lower())\r\n",
        "\r\n",
        "df.news_headline = df.news_headline.apply(html_tag)\r\n",
        "df.news_article = df.news_article.apply(html_tag)\r\n",
        "\r\n",
        "df.news_headline = df.news_headline.apply(con)\r\n",
        "df.news_article = df.news_article.apply(con)\r\n",
        "\r\n",
        "df.news_headline = df.news_headline.apply(remove_sp)\r\n",
        "df.news_article = df.news_article.apply(remove_sp)\r\n",
        "\r\n",
        "df.news_headline = df.news_headline.apply(remove_stopwords)\r\n",
        "df.news_article = df.news_article.apply(remove_stopwords)\r\n",
        "\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUDqt2uy-P0P"
      },
      "source": [
        "# dataset labeling and processing\r\n",
        "vs =  SentimentIntensityAnalyzer()\r\n",
        "df['compound'] = df['news_headline'].apply(lambda x: vs.polarity_scores(x)['compound'])\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyJg1hRQtvOs"
      },
      "source": [
        "# data finalization \r\n",
        "def predict(comp):\r\n",
        "  comp=float(comp)\r\n",
        "  if (comp>0):\r\n",
        "    return 'positive'\r\n",
        "  else:\r\n",
        "    return 'negative'\r\n",
        "df['type_pred'] = df['compound'].apply(predict)\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIGYvZKjnt4O"
      },
      "source": [
        "# saving data to csv\r\n",
        "df.to_csv('news.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH9iFpm2oiQj"
      },
      "source": [
        "# taking ready dataset from Git Hub\r\n",
        "df=pd.read_csv('https://raw.githubusercontent.com/kvora125/Sentiment_Analysis_For_News_Headline-Major_Project/main/dataset/news.csv')\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrG-EFR1wN4t"
      },
      "source": [
        "# dropping not required data and re-indexing the remaining data\r\n",
        "df = df.drop(columns=['news_category'], axis=1)\r\n",
        "df = df.drop(columns=['news_article'], axis=1)\r\n",
        "df = df.set_index(np.arange(len(df)))\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyQ1LI542t-M"
      },
      "source": [
        "# removing punctuations\r\n",
        "punctuations = list(string.punctuation)\r\n",
        "df.news_headline = df.news_headline.apply(lambda x: \" \".join(x for x in x.split() if x not in punctuations))\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjihuOcFDNGu"
      },
      "source": [
        "# Stage 4: Model Building"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rs-p2zfm3K30"
      },
      "source": [
        "# Conversion into Vectors using TFIDF\r\n",
        "x = df.iloc[:,0].values\r\n",
        "y = df.iloc[:,2].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUCQkcTK3Tlk"
      },
      "source": [
        "# splitting data for training and testing\r\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=0)\r\n",
        "np.unique(x_train,return_counts=True)\r\n",
        "np.unique(x_test,return_counts=True)\r\n",
        "np.unique(y_train,return_counts=True)\r\n",
        "np.unique(y_test,return_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slD753D63wce"
      },
      "source": [
        "# model build and train\r\n",
        "final = Pipeline([('Vect',TfidfVectorizer()),\r\n",
        "                  ('model',SVC())])\r\n",
        "final.fit(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-Am3loWDf2W"
      },
      "source": [
        "# Stage 5 : Checking accuracy of the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssSg_CKa4CF-"
      },
      "source": [
        "# testing the model using test data\r\n",
        "y_pred = final.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ4QEuX45J15"
      },
      "source": [
        "# getting accuracy score and other reports\r\n",
        "accuracy_score(y_pred,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXnZGoiM5Lqe"
      },
      "source": [
        "confusion_matrix(y_pred,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzyqicxG5Nq2"
      },
      "source": [
        "print(classification_report(y_pred,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y32RriO8c9t"
      },
      "source": [
        "# Save the Model\r\n",
        "pickle.dump(final,open('sentiment_model.p','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA8QOgdb865L"
      },
      "source": [
        "#runnig saved model from GitHub repo\r\n",
        "import pickle\r\n",
        "if(os.path.isdir(\"Sentiment-Analysis-Major-Project\")):\r\n",
        "  print(\"repo exists\")\r\n",
        "else:\r\n",
        "  Repo.clone_from(\"https://github.com/kvora125/Sentiment-Analysis-Major-Project\", \"/content/Sentiment-Analysis-Major-Project\")\r\n",
        "final=pickle.load(open('/content/Sentiment-Analysis-Major-Project/sentiment_model.p','rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6N5225t6JQR"
      },
      "source": [
        "# testing on random data\r\n",
        "print(final.predict(['₹100 fine for not wearing mask in Indore, 6 found infected with UK COVID variant'])[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YlYjQZ8SZj4"
      },
      "source": [
        "# standalone python deployment to run on custom data and predict (you can directly run this cell)\r\n",
        "!pip install gitpython\r\n",
        "import os\r\n",
        "from git import Repo\r\n",
        "import pickle\r\n",
        "if(os.path.isdir(\"Sentiment-Analysis-Major-Project\")):\r\n",
        "  print(\"repo exists\")\r\n",
        "else:\r\n",
        "  Repo.clone_from(\"https://github.com/kvora125/Sentiment-Analysis-Major-Project\", \"/content/Sentiment-Analysis-Major-Project\")\r\n",
        "final=pickle.load(open('/content/Sentiment-Analysis-Major-Project/sentiment_model.p','rb'))\r\n",
        "# print(final.predict(['₹100 fine for not wearing mask in Indore, 6 found infected with UK COVID variant'])[0])\r\n",
        "predicted = (final.predict([str(input('enter a news headline for sentiment analysis : '))])[0])\r\n",
        "print('Headline is Predicted to be                  :',predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSfQHtIvRceA"
      },
      "source": [
        "# stage 6: Deployment as webapp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEHuHooORvgl"
      },
      "source": [
        "%%writefile app.py\r\n",
        "import streamlit as st\r\n",
        "import os\r\n",
        "from git import Repo\r\n",
        "import pickle\r\n",
        "if(os.path.isdir(\"Sentiment-Analysis-Major-Project\")):\r\n",
        "  print(\"repo exists\")\r\n",
        "else:\r\n",
        "  Repo.clone_from(\"https://github.com/kvora125/Sentiment-Analysis-Major-Project\", \"/content/Sentiment-Analysis-Major-Project\")\r\n",
        "sentiment_model=pickle.load(open('/content/Sentiment-Analysis-Major-Project/sentiment_model.p','rb'))\r\n",
        "st.title(\"News Headline Sentiment Analysis\")\r\n",
        "st.subheader('This project is based on Vader sentiment Analysis(lexicon approach) with TFIFD Vectorizer and SVM to predict sentiment in news healdline')\r\n",
        "st.write('This project uses data Web Scraped from inshorts.com and dataset built using prediction by vader sentiment analysis')\r\n",
        "message = st.text_area(\"Enter News Headline\",\"\")\r\n",
        "if st.button(\"Predict\"):\r\n",
        "  st.title(sentiment_model.predict([message])[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlfOo1qIYwKO"
      },
      "source": [
        "#WebApp Deployment with ngrok server\r\n",
        "!pip install gitpython\r\n",
        "!pip install streamlit\r\n",
        "!pip install pyngrok\r\n",
        "from pyngrok import ngrok\r\n",
        "!nohup streamlit run --server.port 80 app.py >/dev/null &\r\n",
        "url=ngrok.connect(port='80')\r\n",
        "url"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}